# 4. O HIPERTEXTO E AS ESTRUTURAS DIGITAIS

> O sonho de uma biblioteca que reúna todos os saberes acumulados atravessou a história da nossa civilização desde a mítica Biblioteca de Alexandria, na Antiguidade Clássica, passando pelo projeto de Mallarmé, que há um século pensava criar um livro integral (O livro), infinito, síntese de todos os livros passados e por vir. (PARENTE, 1999, p. 68) 

O sonho de uma “biblioteca infinita” habita a mente dos homens há séculos. Assumindo diversos nomes e formas, este sonho enseja uma fonte infinita de conhecimento, que permitiria acesso irrestrito à produção escrita da humanidade; uma biblioteca sem fronteiras, que permitiria difundir o conhecimento entre culturas e acelerar o desenvolvimento dos povos.
No século XX, este sonho foi levado adiante sob a égide do termo “hipertexto”. Cunhado em 1965 no contexto do desenvolvimento de novas tecnologias informáticas no Vale do Silício (WOLF, 1995, cap. 3), este neologismo hoje agrupa diversos conceitos, sistemas e programas (Parente, 1999, p. 80). Dentro deste paradigma, o hipertexto pode ser considerado um "sistema para “estruturação e recuperação da informação de forma multissensorial, dinâmica e interativa” (Idem).
Dado o desenvolvimento alcançado pelos sistemas informáticos hipertextuais desde então, não é de se admirar que estes sejam possam ser considerados como “o último capítulo da história da escrita e do livro” (Idem).[^25] Esta afirmação deixa clara a condição do hipertexto como tecnologia intelectual inserida na linhagem do livro impresso. Não por acaso, o hipertexto resgata e desenvolve algumas das interfaces da escrita. (LEVY, 1993, p. 34).
Contudo, esta herança não significa que o hipertexto deva ser compreendido como mais uma transformação gradual da tecnologia do livro. Ainda que os sistemas hipertextuais importem elementos do livro impresso como índices, sumários e referências cruzadas (Ibid., p. 37), a passagem para o amb-iente hipertextual trata-se muito mais de um corte do que um deslizamento gradual (CHARTIER, 1998, p. 12). O principal atributo intrínseco do hipertexto que permite este corte seria sua velocidade. Para Levy, “a quase instantaneidade da passagem de um nó a outro (...) permite generalizar e utilizar em toda sua extensão o princípio da não linearidade” (1993, p. 37). A altíssima velocidade de navegação no campo do hipertexto leva a um rompimento com a estrutura essencialmente uniforme (mas não linear) do texto impresso, permitindo interações “sob a forma de diagramas, de redes ou de mapas conceituais manipuláveis e dinâmicos” (Ibid., p. 40). 
Levy considera a biblioteca como uma espécie de “megadocumento” bem sinalizado, onde o leitor se desloca (fisicamente) para encontrar os textos que deseja (Ibid., p. 35). O hipertexto, por sua vez, se manifestaria não como um grupo de microtextos que se recombinam rapidamente, mas sim como um “grande metatexto de geometria variável, com gavetas, com dobras” (Ibid., p. 41). Se a conexão entre os textos de uma biblioteca é conceitual, notas de rodapé e referências bibliográficas como linhas invisíveis que unem textos e autores, a conexão entre os textos nas redes hipertextuais é instantânea e *virtual*[^26], fundindo de forma iônica aquilo que antes era irremediavelmente separado por capas e prateleiras.

> Um mapa global não estaria arriscado a tornar-se ilegível a partir de uma certa quantidade de conexões, a tela cobrindo-se de linhas entrecruzadas, em meio as quais não seria possível distinguir mais nada? (LEVY, 1993, p. 38)

O mapa global descrito por Levy não está distante do contexto informacional em que vivemos hoje — de acordo com estimativas, entre 2005 e 2020, a quantidade de dados presente na internet irá se multiplicar por um fator de 300 (IDC, 2012). Este aumento na velocidade, volume e complexidade de textos transmitidos através de redes informáticas não é acompanhado por nossas capacidades cognitivas. Para isto, serão necessárias novas interfaces mediadoras, que gerenciem e otimizem o acesso a este vasto mundo de informação.
No restante deste capítulo serão abordados alguns dos projetos que exploraram as aplicações conceituais e práticas do hipertexto ao longo do século XX. Embora alguns destes projetos apresentem uma natureza programática que à primeira vista pouco tem a ver com o livro e suas práticas, trata-se de um desvio menor do que aparenta, pois neles estão os fundamentos técnicos essenciais dos sistemas hipertextuais e dos chamados “livros digitais”. Obviamente, a seleção aqui apresentada é muito limitada e de nenhuma maneira faz justiça a todos os inventores responsáveis pelo desenvolvimento dos sistemas que hoje temos à disposição — tampouco trata-se de uma linha evolutiva onde cada projeto é sucessor imediato do anterior. Espera-se que o conjunto de projetos descritos possa oferecer uma visão coerente e esclarecedora, que contribua para as análises posteriores e para a visão teórica deste trabalho.
Gutenberg passou anos regulando suas prensas, suas tintas e suas ligas de chumbo e estanho, todos aparatos essenciais para a concretização do livro impresso (LEVY, 1993, p. 183). Assim também passaram-se os anos para os inventores da computação e dos sistemas do hipertexto: soldando circuitos, conectando placas e inventando lógicas.

## 4.1 MEMEX
> O mundo alcançou uma era de dispositivos baratos, complexos e confiáveis; e algo tem que sair disso
> Vannevar Bush, “As We May Think”, 1945

Em 1° Julho de 1945, o engenheiro e administrador Vannevar Bush, então diretor do Centro Norte-Americano para Pesquisa e Desenvolvimento Científico, publicava o artigo “As We May Think”, na revista The Atlantic[^27]. Como responsável pelo direcionamento do desenvolvimento científico norte-americano após a guerra, já naquela época Bush reconhecia um problema no excesso de informação disponível para os acadêmicos norte-americanos. Em seu artigo, chama por “uma nova relação entre o homem pensante e a soma de nossos conhecimentos”, uma mudança necessária uma vez que “a publicação foi extendida muito além de nossa atual capacidade de tirar real proveito das informações” (BUSH, 1945).
Valendo-se de tecnologias então emergentes, como o microfilme, a “fotografia seca”[^28] e o reconhecimento de voz, Bush imagina um sistema que permitiria um melhor gerenciamento deste excesso de dados. Tal sistema não teria o objetvio de substituir o pensamento criativo, mas sim de agilizar e organizar a informação, já que, para esta, “(...) pode haver poderosos auxílios mecânicos” (Idem).
A ferramenta proposta por Bush é um dispositivo chamado de Memex, um aparelho onde o usuário poderia armazenar “todos os seus livros, registros, comunicações, sendo mecanizado de forma a poder ser consultado com alta velocidade e flexibilidade. É um suplemento íntimo à sua memória.” 
O Memex possuiria a aparência de uma mesa, sobre a qual duas telas translúcidas projetariam o material armazenado em microfilme. Haveria também uma chapa transparente, sobre a qual livros, textos, imagens e outros materiais seriam colocados para serem fotografados e adicionados ao Memex, além de um teclado e botões para navegação.
Diferente de outros sistemas de armazenamento existentes à época, o Memex se destacaria por selecionar os dados de forma associativa, ao invés de indexada, mais em linha com a forma de funcionamento da mente humana, que associa pensamentos de forma não-linear através da sugestão, formando com eles uma *intrincada rede de trilhos*:

> É exatamente como se itens físicos fossem unidos de fontes completamente distintas e costurados juntos para formar um novo livro. É mais do que isto, já que qualquer item pode ser vinculado à mais de um trilho. (BUSH, 1945)

De fato, Bush define esta como a principal funcionalidade do Memex: a capacidade de associar dois ou mais itens e criar uma trilha associativa que liga todos os pedaços da informação de maneira não-hierárquica. O usuário do Memex não estaria limitado apenas a organizar e classificar informação, teria também a possibilidade de adicionar suas próprias notas aos textos já registrados, criando, através destas trilhas de textos próprios e de terceiros, uma vasta enciclopédia pessoal.

## 4.2 “A MÃE DE TODAS AS DEMOS”

> Se em seu escritório, você, como um trabalhador intelectual, tivesse uma tela conectada a um computador ligado durante todo o dia, e que respondesse instantâneamente a qualquer ação que você conduzisse, quanto valor você poderia derivar disto? (ENGELBART, 1968).
> Douglas Engelbart, na introdução da apresentação “The Mother of all Demos”

O pesquisador norte-americano Douglas Engelbart ficou conhecido por realizar em 1968 a apresentação de um novo sistema informático que mudaria o rumo da história da computação. Na apresentação que viria a ser conhecida como “a mãe de todas as demos”, Engelbart fez a demonstração do sistema então chamado NLS (oNLine System), contendo uma série de tecnologias revolucionárias que só viriam a ser amplamente difundidas anos depois. Algumas das funcionalidades do NLS, incluíam[^29]:

 - uso de múltiplas janelas de trabalho;
 - interface gráfica operada por um “mouse”;
 - conexões hipertextuais entre diferentes documentos;
 - vídeo-conferência com compartilhamento de tela;
 - edição remota e controle de versão para documentos.

Estas funcionalidades, de modo geral, são lugar comum no atual contexto da internet. Mesmo assim, ao considerarmos que um dos serviços mais populares de colaboração remota, Google Docs, foi lançado em *2005*[^30], a demonstração de Engelbart em 1968 exibe uma visão muito exata dos problemas que a informática viria a resolver. Mesmo nos dias de hoje, algumas das soluções propostas por Engelbart não foram totalmente implementadas: seu sistema de edição de texto, que permitia diferentes níveis de visualização do conteúdo de um mesmo texto, é mais complexo do que o modelo adotado por editores de texto como Google Docs e Microsoft Word, cuja visualização principal replica o tratamento linear do texto encontrado em uma máquina de escrever.
À época atuando como diretor do ARC (Augmentation Research Center), a missão declarada de Doug Engelbart era “‘aumentar’ o funcionamento dos grupos” (LEVY, 1993, p. 51) utilizando a colaboração em rede como ferramenta da mudança. Suas premissas sobre a capacidade da informática (à época revolucionária) de remanejar o espaço de trabalho eram muito alinhadas com o pensamento de Levy, quando este diz que:

> “os diversos agenciamentos de mídias, tecnologias (...) e métodos de trabalho disponíveis em uma dada época condicionam fundamentalmente a maneira de pensar e funcionar em grupo vigente numa sociedade.” (Ibid., pp. 52-53)

Assim, mais do que desenvolver tecnologias visando o puro avanço da técnica, Engelbart tinha como preocupação levar o aumento de possibilidades de colaboração para qualquer tipo de trabalhador, em qualquer escritório, através de uma relação com o computador que fosse, novamente nas palavras de Levy, “intuitiva, metafórica e sensório-motora” (Ibid., p. 52). Não por acaso, o ARC foi responsável pelo desenvolvimento da interface gráfica e do mouse, duas das invenções que impulsionaram o salto para o surgimento da indústria dos computadores pessoais nas duas décadas seguintes.
Contudo, sua visão não era compartilhada por seus contemporâneos, para os quais “a informática ainda era tida como uma arte de automatizar cálculos, e não como uma tecnologia intelectual” (Ibid., p. 51). O projeto de Engelbart nunca foi devidamente finalizado, pois o ARC perdeu financiamento do governo alguns anos depois. Engelbart tentou ainda continuar o desenvolvimento do NLS dentro de empresas privadas, mas nunca recebeu o espaço e os recursos necessários para levar adiante sua pesquisa (O’BRIEN, 2013).
Sua equipe original, porém, levou consigo algumas das tecnologias desenvolvidas no ARC e, dentro de empresas como Xerox, Apple e Sun Microsystems, iniciaram a revolução do computador pessoal que herdou a visão de informática de Engelbart (Op. cit.) — uma informática “encarregada dos equipamentos coletivos da inteligência, contribuindo para estruturar os espaços cognitivos dos indivíduos e das organizações” (Ibid., p. 53).
 
## 4.3 XANADU

O sociólogo e pesquisador norte-americano Ted Nelson é creditado como pai do termo “hipertexto”, tendo usado-o pela primeira vez em um artigo apresentado na conferência nacional da “Association for Computing Machinery” em 1965 (WOLF, 1995).
Nelson usou o conceito de hipertexto como base para seu projeto “Xanadu”. Um visionário projeto de software, Xanadu tinha como principais objetivos criar uma “biblioteca universal, um index global de informações e um sistema de royalties computadorizado” (Idem).
Em 1974, Nelson publicaria sua obra prima, um enorme manifesto de 300.000 palavras sobre a revolução digital composto por duas obras costa-a-costa, “Computer Lib” e “Dream Machines” (Idem). Numa passagem de Dream Machines, Nelson define Xanadu como um espaço onde, através da rede, documentos seriam linkados e comparados, criando um “‘docuverso’ capaz de armazenar e representar o legado artístico e científico da humanidade”. A terminologia grandiosa não é exagero: através de uma funcionalidade chamada “transclusão”, usuários de Xanadu poderiam linkar partes de documentos através deste ‘docuverso’ sem nenhuma redundância — isto é, nenhuma parte de nenhum texto seria jamais duplicada, apenas ‘transclusa’ (Idem). Esta funcionalidade apresentava um grande desafio técnico para os programadores, pois significava que “todos os usuários do mundo deveriam ter acesso instantâneo à mesma coleção de documentos na base” (Idem).
  Em parte por seu escopo visionário e mal definido, Xanadu nunca foi lançado como um produto finalizado. Entre outros fatores, a empreitada estava limitada pelo hardware de sua época: em 1979, a equipe de Xanadu tinha a missão de programar “uma biblioteca universal em máquinas que mal conseguiam editar e buscar o texto presente em um único livro” (Idem). John Walker, fundador da Autodesk (que na década de 80 financiou o projeto), se disse descrente de que a equipe de Xanadu poderia criar “por completo, um sistema que pode armazenar toda a informação em todas as suas formas, presente e futuras” e que este processo “sempre falha”. Ao invés de criar um sistema flexível, que pudesse ser lançado e evoluir conforme as necessidades dos usuários e do mercado, a equipe Xanadu, em parte por ambição cega, em parte pelo escopo faraônico do projeto, queria “lançar sua revolução *ab initio*”.
Depois de mais de quatro décadas, diversas equipes participando e empresas financiando o projeto, os únicos produtos finais seriam três protótipos que nunca foram a público. A visão de Nelson, porém, formou uma geração de *hackers* e programadores, empoderando-os de uma responsabilidade que até então poucos compreendiam: ao disponibilizar informação de forma ampla e através de uma plataforma tão radical, o projeto Xanadu pretendia “eliminar a ignorância científica e curar desentendimentos políticos” — uma linha de pensamento que leva à conclusão messiânica de que a missão Xanadu seria, de fato, salvar o mundo. (WOLF, 1995).
A [página da web oficial do projeto](http://www.xanadu.com) apresenta, além de uma breve descrição histórica e técnica do projeto, um inflamado discurso político que justifica algumas das dificuldades enfrentadas pelo projeto. No topo, os dizeres: “com ideias ainda radicais, CONTINUAMOS NA LUTA” (maiúscula original). O documento de especificação mais recente[^31] data de [Março de 2012](em http://xanadu.com/xanasimp).

## 4.4 WORLD WIDE WEB

> Em parte, a World Wide Web é a primeira implementação prática de um sistema de hipertexto global tal como imaginado pelos pioneiros do hipertexto.

> (BERNERS-LEE, 1993/1994b)

Desenvolvida a partir de 1989 por Tim Berners-Lee, à época pesquisador no CERN[^32], a World Wide Web (www, W3 ou web) foi até hoje o projeto de sistema de hipertexto mais bem-sucedido. De fato, a web foi tão universalmente adotada que se tornou, para um usuário leigo, sinônimo da própria Internet[^33].
A web surgiu com o objetivo inicial de facilitar a colaboração remota entre pesquisadores do CERN, evitando perda de documentos, dados de experimentos e outras informações em meio à alta rotatividade de pesquisadores no instituto. Demonstrando uma visão precisa sobre o problema do excesso de informação, há muito destacado por Vannevar Bush, Berners-Lee afirmava na especificação inicial da www que o “CERN se depara hoje com problemas que o restante do mundo terá de enfrentar em breve” (BERNERS-LEE, 1989).
Sua especificação inicial descrevia um sistema de organização de documentos hipertextual não-hierárquico, em rede (daí o termo “web”), que permitiria a inserção de qualquer tipo de conteúdo. Tal rede seria composta por nós — que poderiam ser notas, comentários, documentos, etc — conectados por *links* (BERNERS-LEE, 1989). A arquitetura da web segue um modelo onde *browsers* (navegadores) se conectam a *servers* (servidores), onde uma ponta pode desconhecer o sistema operacional ou formato de dados da outra, pois são intermediados por uma interface HTTP, que traduz quaisquer diferenças de linguagem entre os dois. (BERNERS-LEE e CAILLAU, 1992)
O projeto inicial de Tim Berners-Lee inclui ainda algumas premissas básicas, postas como pré-requisitos diante das necessidades do CERN, mas que se provariam fundamentais para o desenvolvimento e sucesso do sistema (BERNERS-LEE, 1989): 

  - **Descentralização do conteúdo**, permitindo que novos sistemas possam se conectar sem necessitar uma autoridade central;
  - **Heterogeneidade**, permitindo que diversos sistemas operacionais possam acessar a web;
  - **Acesso à dados já existentes**, reinterpretando-os para um formato de hipertexto, o que impulsionaria a adoção da web;
  - **Links “live”**, permitindo que documentos em hipertexto sejam linkados a informações atualizadas em tempo real;
  - E ainda **“não requerimentos”**, onde Berners-Lee afirma que questões técnicas a respeito do suporte de copyright e proteção de dados, mesmo sendo viáveis, não seriam propostas naquele documento.

Este último ponto reflete um posicionamento ideológico muito importante de Tim Berners-Lee. Em um artigo posterior[^34], ele descreve a web como “um universo aberto de informação distribuída através de uma interface hipertextual (BERNERS-LEE, 1993/1994b). O aspecto aberto da web permitiu sua rápida difusão para além do meio acadêmico: por meio de “gateways”, a web permitia a conexão com sistemas já existentes; a facilidade de criar um servidor www, que não requeria registro central, possibilitou à rede um crescimento exponencial[^35]; e a disponibilização de todo o código-fonte pelo CERN em licença livre (*open source*) fez com que rapidamente, estudantes e programadores de diferentes partes do mundo criassem browsers de acesso à web para a maioria dos sistemas operacionais disponíveis à época (BERNERS-LEE, 1993/1994a)[^36].
Tal difusão não foi apenas um empurrão para a adoção da web, mas crucial para sua sobrevivência como sistema: segundo Berners-Lee, a www não era tão amplamente adotada dentro do CERN, sendo portanto “pessoas interessadas na internet que providenciavam o feedback, estímulo, ideias, contribuições de código-fonte (...) as pessoas na internet construíram a web.” (Idem). Este movimento não foi acidental, já que, tendo pensado um sistema modular e descentralizado que permitia (mais que isto, requeria) amplas contribuições externas, Tim Berners-Lee esperava que a web “alcançasse muito cedo uma utilidade crítica” (BERNERS-LEE, 1989), de forma que a própria utilidade do sistema iria encorajar a participação de novos usuários.
Por mais que a atual ubiquidade da web seja prova auto-evidente do sucesso desta estratégia, à época, outros sistemas seguiam o caminho oposto. Em 1993, a Universidade de Minnesota anunciou que passaria a cobrar uma licença pelo uso comercial de seu protocolo de compartilhamento de documentos online Gopher (“Minnesota Gopher Team”, 1993), concorrente da web, em um movimento que ajudou a impulsionar a adoção e desenvolvimento das tecnologias da www. Apenas sete anos depois o código-fonte do projeto Gopher foi liberado para uso comercial, mas então já era tarde e o lançamento gerou pouco interesse, pois o protocolo se encontrava em desuso (KOSTECKE, 2000). 
Similarmente, o sistema Xanadu, que nunca foi lançado, jamais teve seu código-fonte compartilhado, por ser marca registrada do Projeto Xanadu. Uma das invenções do projeto, uma estrutura de dados chamada enfilade, nunca teve sua especificação técnica publicada. Segundo Ted Nelson, líder do projeto original, o motivo para isto seria por que a invenção “ainda é uma *parada quente*” — nosso itálico (WOLF, 1995).
O trabalho inicial de Tim-Berners Lee e diversos outros colaboradores criaram as tecnologias básicas da web, entre elas o HTML (Hypertext Markup Language), sintaxe para visualização hipertextual de conteúdo durante a navegação da web; o servidor HTTP (Hypertext Transfer Protocol), que rege a comunicação cliente-servidor na web; os primeiros browsers, que permitem comunicação com os servidores da web através de linhas de código ou interfaces gráficas; e as URLs (Uniform Resource Locator), protocolo de endereçamento universal que permite aos usuários da web acessarem recursos e documentos em diferentes sistemas.
Este legado técnico foi adotado pelo W3C, World Wide Web Consortium, fundado em 1994 numa parceria entre CERN e DARPA[^37], sendo encabeçado pelo próprio Tim Berners-Lee (W3C a). Com um corpo técnico hoje formado por cientistas e desenvolvedores de empresas e instituições acadêmicas de todo o mundo[^38], a W3C tem como missão levar adiante a Web respeitando os valores de abertura e acessibilidade que a acompanham desde sua incepção, desenvolvendo protocolos e diretrizes que garantam o crescimento unificado da Web no longo prazo (W3C b).

[^25]: Vale notar que estas considerações foram feitas por André Parente em 1999, num momento em que a Web ainda não estava totalmente consolidada e que os micro-computadores (smartphones, tablets, entre outros) ainda engatinhavam se comparados ao espaço que ocupam em 2013.

[^26]: O que não significa de forma alguma que não sejam reais: “A noção do offline como real e autêntico é uma invenção recente, que corresponde com o crescimento do online. Se pudermos consertar esta falsa separação e ver o digital \[virtual\] e o físico como entrelaçados, então entenderemos que o que fazemos enquanto conectados é inseparável do que fazemos quando desconectados.” (JURGUENSEN, 2012).

[^27]: Disponível em http://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/. Acessado em 01 de Novembro de 2013.

[^28]: Processo de fotografia com revelação em gel, que dispensava o uso de líquidos químicos para revelação.

[^29]: Os três primeiros tópicos são mencionados por Pierre Levy (1993, p. 51), enquanto os dois últimos aparecem no artigo da revista WIRED de 2008 (TWENEY, 2008). O vídeo completo pode ser visto em (ENGELBART, 1968).

[^30]: Segundo a página do serviço na Wikipedia, disponível no endereço http://pt.wikipedia.org/wiki/Google_Docs. Acessada em: 5 de Novembro de 2013.

[^31]: Em Janeiro de 2014

[^32]: Organização Européia para a Pesquisa Nuclear

[^33]: Para pautar a diferença, o termo Internet se refere à conexão internacional entre computadores através de cabos telefônicos/de fibra óptica ou via satélite que, utilizando diversos sistemas e protocolos como www, email, FTP e Usenet, constitui um “vasto conglomerado de redes de computadores interconetadas” (BERNERS-LEE, 1993/1994a)

[^34]: Parte de uma série de artigos que Tim Berners-Lee pretendia publicar como um livro sobre o histórico e funcionamento da web, todos escritos entre 1993 e 1994.

[^35]: Tim Berners-Lee afirma que entre Julho de 1991 e Julho de 1994, a rede dobrava de tamanho a cada quatro meses — fator que pulava para 10x, se considerados os 12 meses anteriores. (1993/1994a, capítulo “Proliferation”). 

[^36]: Movimento descrito no capítulo “The Graphic User Interfaces Bloom”

[^37]: Defense Advanced Research Projects Agency, agência do governo norte-americano responsável por promover a pesquisa e desenvolvimento de novas tecnologias voltadas para aplicação militar.

[^38]: Uma lista completa das empresas e instituições participantes pode ser consultada em <http://www.w3.org/Consortium/Member/List>